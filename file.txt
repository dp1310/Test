Here is the cleaned transcript as plain text. You can copy-paste it into a `.txt` file to download or save locally. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/2398668/82388e74-fda5-4ddb-bc48-2d6618ff62f5/Recording-2026-01-19-182234.txt)

***

I am going to talk about a technique called a micro‑agent. Nikit has written a service in a Jupyter notebook where he has not explicitly called out agents, but he is doing parsing and retrieval. He is essentially doing knowledge management and using reasoning to understand whether there needs to be a human in the loop for verification. He is using tools to store the information into a database and validator micro‑agents to validate the output.

He is basically doing a sequence of tasks, but those are not separated into agents. We need to be able to separate concerns here, because there is some model development also. He has to separate the model world and the agent world.

The model is stateless. It works only at a data‑structure and algorithmic level and gives you the output. It does not maintain state.

State, context, workflow and coordination all belong in the agent world, with the help of our framework. The agent is built on top of the framework. What we want to do now is build this service, making a few assumptions.

We are going to build an information‑extraction service that uses multiple agents to extract information. That information‑extraction service is, by default, a multi‑tenant service.

We are going to make certain assumptions before proceeding. We are not going to design the agents that sit on the browser and work directly with the user. We are only going to design the maximum gateway, context‑gateway engines and the subsequent agents that are needed.

The agents will talk to each other using gRPC or A2A messaging, NATS JetStream, etc., where context is a first‑class concept.

In order to extract information, you need to plan your task. This is specialized task planning. You need to be able to parse the document. You need to be able to store that document; the document itself might go into blob storage. You might need to split it and break it into chunks for retrieval. That means there will be RAG‑style agents that help with retrieval.

Then you have actuator agents or reasoning agents that help you extract information, given a knowledge model. You start with a concept model. In his case, he has a base concept model of product development.

A document comes in, he parses it and organizes it hierarchically without using knowledge initially. Then, with each chunk, he organizes or annotates the document using the knowledge model. When the knowledge is not present, he goes to a global knowledge base and says, “I don’t understand this, can you tell me what it is?” The global knowledge base is going to be an LLM.

The reasoning engine is built on top of an LLM, so there is model involvement there. While digesting the document, he may see something like “Diligenta” mentioned. There is no explanation of what Diligenta is in the document, so he goes to the global knowledge base and asks what it is, in a structured way.

The document may also talk about “Alchemist” with no explanation. He has to infer what Alchemist is. Somewhere the document says “TCS Alchemist”, elsewhere “TCS Alchemist platform”, elsewhere “Alchemist platform”, and in some places just “Alchemist”. He needs to know whether these are different entities or the same.

So he has to clarify with the user and say, “These seem to be related, but I do not know how.” A clarification loop is created. The model or reasoning agent asks: “These seem to be related. Tell me how they are related. How do I merge or collapse them?” That is almost like a validator agent because it validates the input and the output.

The model will ask the clarification from the human operator. This is not an end‑user interaction; this is for operator intervention. All of this needs to happen in an asynchronous, non‑blocking, and possibly non‑sequential manner. Each agent carries context.

This is similar to NER‑like activity, but not necessarily the same. Document hierarchical chunking itself can happen in a non‑linear fashion. You can have more than one way of chunking the document.

Knowledge extraction and annotation need not be linear. When it gathers new knowledge, it has to go back to the annotation process and re‑annotate with the new knowledge. It also needs to extract a new knowledge graph from the content, separate from the annotation. It needs somewhere to store that knowledge.

It needs to be able to say, “I am answering using this version of the knowledge.” For each question, it should track which version of the knowledge base was used. Depending on the document you handle, various policies will come into effect.

If you are handling PII‑related data, then policies with respect to redaction, either on the flow or at storage (if using durable messaging), must come into the picture. You still need to be able to say, “I am answering using this version of the knowledge.”

The same architecture can be used for code generation. You will need a planning agent (the manifest planner). You need a validator or decision agent that validates and decides which methods to use. Monitoring agents check policies and validate whether the generated code complies with them.

That is the basic idea. More advanced features can be added later, but the basic setup works and can handle multiple use cases.

Maybe at some point, we will revisit this meeting and take the code from Nikhil so you can, in parallel, look at the code. Remember, his code mixes everything and uses notebooks. That is fine; he was asked not to worry about agents and to concentrate on functionality.

There are two or three more documents we may need to give him. One of them is about the model pipeline. You should already have this document, so you should be able to send it out. It has been mailed.

Now, looking at your current work: this is the components diagram. It shows what components exist: agent, CMP platform, service layer, harness layer, accessibility, and how they interact. Then there is a data‑flow diagram showing how the flow goes from agent request to final result.

Where are you showing the non‑linearity of the flow? The fan‑out/fan‑in you mentioned is not captured in the diagram. Some code is there, but documentation should explicitly talk about non‑linearity. You need to add a section that explains how to design agents for non‑linear flows.

This is a basic flow; the code shows how context is handled. However, you are using a lot of if‑statements. Even the example code should be functional in nature. Instead of if‑statements, use lambdas and predicates. The CMP code base already has a plain stack‑based pattern that can be leveraged.

You were trying to illustrate fan‑out/fan‑in, but it was not documented as a separate section. Add a section for that. What we are creating is a **reference** implementation. The basics should be clean and composable so that more advanced use cases can later be built on top.